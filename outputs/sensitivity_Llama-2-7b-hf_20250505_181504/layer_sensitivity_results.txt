Baseline perplexity: 11.4348

Layer Sensitivity Results (sorted by impact):
1. model.layers.1.mlp.gate_proj.weight: 14508.7353 (delta: 14497.3004)
2. model.layers.0.self_attn.q_proj.weight: 617.3573 (delta: 605.9225)
3. model.layers.0.mlp.gate_proj.weight: 258.5933 (delta: 247.1584)
4. model.layers.3.self_attn.q_proj.weight: 46.1596 (delta: 34.7248)
5. model.layers.2.self_attn.q_proj.weight: 40.7630 (delta: 29.3282)
6. model.layers.4.mlp.gate_proj.weight: 16.2282 (delta: 4.7933)
7. model.layers.4.self_attn.q_proj.weight: 13.8779 (delta: 2.4430)
8. model.layers.2.mlp.gate_proj.weight: 13.8234 (delta: 2.3885)
9. model.layers.1.self_attn.q_proj.weight: 12.2292 (delta: 0.7943)
10. model.layers.3.mlp.gate_proj.weight: 11.7448 (delta: 0.3099)
